{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>institutional_affiliation</th>\n",
       "      <th>metric_id</th>\n",
       "      <th>personal_account_id</th>\n",
       "      <th>search_type</th>\n",
       "      <th>query_string</th>\n",
       "      <th>requestid</th>\n",
       "      <th>auth_sessionid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-08-03</td>\n",
       "      <td>False</td>\n",
       "      <td>Searches_Platform_3d33f627-3ad0-4e7e-9832-7390...</td>\n",
       "      <td>c6cefc2a-6d0a-4f3e-9e1e-7bc6b27ac5f9</td>\n",
       "      <td>basic_search</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14c47cd5-9d31-fee3-cfce-76d9b7214ac3</td>\n",
       "      <td>94f1a3263a6348f1bf7ffbd6d8c39c05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-08-25</td>\n",
       "      <td>True</td>\n",
       "      <td>Searches_Platform_0c60725b-87d3-49ed-bb1b-0b66...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>basic_search</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1525f88f-dcc4-58a7-580a-9e2e2ad06768</td>\n",
       "      <td>818df6122523486181e3619ea694095c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-08-12</td>\n",
       "      <td>False</td>\n",
       "      <td>Searches_Platform_786def01-bdf1-4d6a-adda-c5c8...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>basic_search</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1b4c0339-e625-133a-ced9-8eaa7fe09695</td>\n",
       "      <td>9f98b93cdd964f5fb54363336fded27a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-08-31</td>\n",
       "      <td>True</td>\n",
       "      <td>Searches_Platform_b5f671cd-d299-402b-9067-5092...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>basic_search</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1f9fcf72-f889-466f-8ac8-4e0066d55b16</td>\n",
       "      <td>87d139d379264f61b247bab366411388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-08-25</td>\n",
       "      <td>False</td>\n",
       "      <td>Searches_Platform_35ef3849-f2be-4d8d-8911-3afe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>basic_search</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2307d346-05b1-c826-6a92-f1732ed22fb2</td>\n",
       "      <td>cf99c55f8014455c8ce8b0301741da7e</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  institutional_affiliation  \\\n",
       "0  2023-08-03                      False   \n",
       "1  2023-08-25                       True   \n",
       "2  2023-08-12                      False   \n",
       "3  2023-08-31                       True   \n",
       "4  2023-08-25                      False   \n",
       "\n",
       "                                           metric_id  \\\n",
       "0  Searches_Platform_3d33f627-3ad0-4e7e-9832-7390...   \n",
       "1  Searches_Platform_0c60725b-87d3-49ed-bb1b-0b66...   \n",
       "2  Searches_Platform_786def01-bdf1-4d6a-adda-c5c8...   \n",
       "3  Searches_Platform_b5f671cd-d299-402b-9067-5092...   \n",
       "4  Searches_Platform_35ef3849-f2be-4d8d-8911-3afe...   \n",
       "\n",
       "                    personal_account_id   search_type query_string  \\\n",
       "0  c6cefc2a-6d0a-4f3e-9e1e-7bc6b27ac5f9  basic_search          NaN   \n",
       "1                                   NaN  basic_search          NaN   \n",
       "2                                   NaN  basic_search          NaN   \n",
       "3                                   NaN  basic_search          NaN   \n",
       "4                                   NaN  basic_search          NaN   \n",
       "\n",
       "                              requestid                    auth_sessionid  \n",
       "0  14c47cd5-9d31-fee3-cfce-76d9b7214ac3  94f1a3263a6348f1bf7ffbd6d8c39c05  \n",
       "1  1525f88f-dcc4-58a7-580a-9e2e2ad06768  818df6122523486181e3619ea694095c  \n",
       "2  1b4c0339-e625-133a-ced9-8eaa7fe09695  9f98b93cdd964f5fb54363336fded27a  \n",
       "3  1f9fcf72-f889-466f-8ac8-4e0066d55b16  87d139d379264f61b247bab366411388  \n",
       "4  2307d346-05b1-c826-6a92-f1732ed22fb2  cf99c55f8014455c8ce8b0301741da7e  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/Users/fionaaaaa_mei/Downloads/august_2023_searches.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['query_string'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>institutional_affiliation</th>\n",
       "      <th>metric_id</th>\n",
       "      <th>personal_account_id</th>\n",
       "      <th>search_type</th>\n",
       "      <th>query_string</th>\n",
       "      <th>requestid</th>\n",
       "      <th>auth_sessionid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2023-08-07</td>\n",
       "      <td>True</td>\n",
       "      <td>Searches_Platform_cd11029c-37bf-42f5-8284-1d3f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>basic_search</td>\n",
       "      <td>Child abuse</td>\n",
       "      <td>fastly-default:00032f2cdcc9e112559ce0f44451d869</td>\n",
       "      <td>df2c17513cb64824b246e84cea5475e3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2023-08-29</td>\n",
       "      <td>False</td>\n",
       "      <td>Searches_Platform_af5d01f0-58d4-4153-bda5-c915...</td>\n",
       "      <td>d30ed7fe-6536-4261-9b1c-5d127ca2390e</td>\n",
       "      <td>basic_search</td>\n",
       "      <td>1.\\tAhmad, N., &amp; Hassan, R. (2015). The effect...</td>\n",
       "      <td>fastly-default:0003926a918f974401fce8e9993c3767</td>\n",
       "      <td>89098123304e450295305f421b93f438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2023-08-17</td>\n",
       "      <td>False</td>\n",
       "      <td>Searches_Platform_b1d6a3f7-5bda-489f-821c-4e7e...</td>\n",
       "      <td>e828e177-21ab-4a57-8a85-49789d028ff2</td>\n",
       "      <td>basic_search</td>\n",
       "      <td>gather ye rosebuds while ye may</td>\n",
       "      <td>fastly-default:0007f21549b981eec2f85e013f260023</td>\n",
       "      <td>5bef5573e2f944869b2cd9da411228ec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2023-08-30</td>\n",
       "      <td>True</td>\n",
       "      <td>Searches_Platform_271f3c55-45b6-4f5f-9327-5549...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>basic_search</td>\n",
       "      <td>[Nicolaas van der Waay (1855-1936): A Dutch Pa...</td>\n",
       "      <td>fastly-default:00094239ffc0c12e40e10b2ec18941ce</td>\n",
       "      <td>8a6f7a9476934f01b8baa39abd2967fd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2023-08-21</td>\n",
       "      <td>False</td>\n",
       "      <td>Searches_Platform_1659c830-ca36-499b-bfe7-b176...</td>\n",
       "      <td>3bd26986-75de-4dfc-8c03-7847910d7353</td>\n",
       "      <td>basic_search</td>\n",
       "      <td>car pollution</td>\n",
       "      <td>fastly-default:000ff57e598d821597179d943571f3ce</td>\n",
       "      <td>3442c7b38f5148339a27c4fc0fca6f91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            dt  institutional_affiliation  \\\n",
       "24  2023-08-07                       True   \n",
       "25  2023-08-29                      False   \n",
       "26  2023-08-17                      False   \n",
       "27  2023-08-30                       True   \n",
       "28  2023-08-21                      False   \n",
       "\n",
       "                                            metric_id  \\\n",
       "24  Searches_Platform_cd11029c-37bf-42f5-8284-1d3f...   \n",
       "25  Searches_Platform_af5d01f0-58d4-4153-bda5-c915...   \n",
       "26  Searches_Platform_b1d6a3f7-5bda-489f-821c-4e7e...   \n",
       "27  Searches_Platform_271f3c55-45b6-4f5f-9327-5549...   \n",
       "28  Searches_Platform_1659c830-ca36-499b-bfe7-b176...   \n",
       "\n",
       "                     personal_account_id   search_type  \\\n",
       "24                                   NaN  basic_search   \n",
       "25  d30ed7fe-6536-4261-9b1c-5d127ca2390e  basic_search   \n",
       "26  e828e177-21ab-4a57-8a85-49789d028ff2  basic_search   \n",
       "27                                   NaN  basic_search   \n",
       "28  3bd26986-75de-4dfc-8c03-7847910d7353  basic_search   \n",
       "\n",
       "                                         query_string  \\\n",
       "24                                        Child abuse   \n",
       "25  1.\\tAhmad, N., & Hassan, R. (2015). The effect...   \n",
       "26                    gather ye rosebuds while ye may   \n",
       "27  [Nicolaas van der Waay (1855-1936): A Dutch Pa...   \n",
       "28                                      car pollution   \n",
       "\n",
       "                                          requestid  \\\n",
       "24  fastly-default:00032f2cdcc9e112559ce0f44451d869   \n",
       "25  fastly-default:0003926a918f974401fce8e9993c3767   \n",
       "26  fastly-default:0007f21549b981eec2f85e013f260023   \n",
       "27  fastly-default:00094239ffc0c12e40e10b2ec18941ce   \n",
       "28  fastly-default:000ff57e598d821597179d943571f3ce   \n",
       "\n",
       "                      auth_sessionid  \n",
       "24  df2c17513cb64824b246e84cea5475e3  \n",
       "25  89098123304e450295305f421b93f438  \n",
       "26  5bef5573e2f944869b2cd9da411228ec  \n",
       "27  8a6f7a9476934f01b8baa39abd2967fd  \n",
       "28  3442c7b38f5148339a27c4fc0fca6f91  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stop words and rejoin list of words\n",
    "#Lifted from https://medium.com/@yashj302/stopwords-nlp-python-4aa57dc492af\n",
    "\n",
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
    "              'ourselves', 'you', 'your', 'yours', 'yourself',\n",
    "              'yourselves', 'he', 'him', 'his', 'himself', 'she',\n",
    "              'her', 'hers', 'herself', 'it', 'its', 'itself',\n",
    "              'they', 'them', 'their', 'theirs', 'themselves',\n",
    "              'what', 'which', 'who', 'whom', 'this', 'that',\n",
    "              'these', 'those', 'am', 'is', 'are', 'was', 'were',\n",
    "              'be', 'been', 'being', 'have', 'has', 'had', 'having',\n",
    "              'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and',\n",
    "              'but', 'if', 'or', 'because', 'as', 'until', 'while',\n",
    "              'of', 'at', 'by', 'for', 'with', 'about', 'against',\n",
    "              'between', 'into', 'through', 'during', 'before',\n",
    "              'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "              'in', 'out', 'on', 'off', 'over', 'under', 'again',\n",
    "              'further', 'then', 'once', 'here', 'there', 'when',\n",
    "              'where', 'why', 'how', 'all', 'any', 'both', 'each',\n",
    "              'few', 'more', 'most', 'other', 'some', 'such', 'no',\n",
    "              'nor', 'not', 'only', 'own', 'same', 'so', 'than',\n",
    "              'too', 'very', 'should', 'can', 'now', 'will', 'just',\n",
    "              'would', 'could', 'may', 'must', 'one', 'much', \"it's\",\n",
    "              \"can't\", \"won't\", \"don't\", \"shouldn't\", \"hasn't\"]\n",
    "\n",
    "#\n",
    "                              'ourselves', 'you', 'your', 'yours', 'yourself',\n",
    "                              'yourselves', 'he', 'him', 'his', 'himself',\n",
    "                              'she', 'her', 'hers', 'herself', 'it', 'its',\n",
    "                              'itself', 'they', 'them', 'their', 'theirs',\n",
    "                              'themselves','which', 'who', 'whom', 'this',\n",
    "                              'that', 'these', 'those', 'what', 'a', 'an',\n",
    "                              'the', 'and', 'but', 'if', 'or', 'because',\n",
    "                              'as', 'of', 'at', 'by', 'for', 'with', 'about',\n",
    "                              'against', 'between', 'into', 'through', 'above',\n",
    "                              'below', 'to', 'from', 'up', 'down', 'in', 'out',\n",
    "                              'on', 'off', 'over', 'under', 'again', 'further',\n",
    "                              'here', 'there', 'all', 'any', 'both', 'each',\n",
    "                              'few', 'more', 'most', 'other', 'some', 'such',\n",
    "                              'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "                              'than', 'too', 'very', 'one', 'much', \"it's\",\n",
    "                              \"can't\", \"won't\", \"don't\", \"shouldn't\", \"hasn't\"]\n",
    "\n",
    "#set for memory optimization\n",
    "stopwords = set(stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(string):\n",
    "    return ''.join(char.lower() for char in string if char.isalpha() or char.isspace())\n",
    "\n",
    "def remove_word(string):\n",
    "    words = string.split()\n",
    "    return ' '.join(word for word in words if word not in stopwords_temporal_removed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hate'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_word(clean_string(\"A hate you\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i hate'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_string(remove_word(\"I Hate you\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['query_string'] = df['query_string'].apply(clean_string).apply(remove_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>institutional_affiliation</th>\n",
       "      <th>metric_id</th>\n",
       "      <th>personal_account_id</th>\n",
       "      <th>search_type</th>\n",
       "      <th>query_string</th>\n",
       "      <th>requestid</th>\n",
       "      <th>auth_sessionid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2023-08-07</td>\n",
       "      <td>True</td>\n",
       "      <td>Searches_Platform_cd11029c-37bf-42f5-8284-1d3f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>basic_search</td>\n",
       "      <td>child abuse</td>\n",
       "      <td>fastly-default:00032f2cdcc9e112559ce0f44451d869</td>\n",
       "      <td>df2c17513cb64824b246e84cea5475e3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2023-08-29</td>\n",
       "      <td>False</td>\n",
       "      <td>Searches_Platform_af5d01f0-58d4-4153-bda5-c915...</td>\n",
       "      <td>d30ed7fe-6536-4261-9b1c-5d127ca2390e</td>\n",
       "      <td>basic_search</td>\n",
       "      <td>ahmad n hassan r effects external funding sust...</td>\n",
       "      <td>fastly-default:0003926a918f974401fce8e9993c3767</td>\n",
       "      <td>89098123304e450295305f421b93f438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2023-08-17</td>\n",
       "      <td>False</td>\n",
       "      <td>Searches_Platform_b1d6a3f7-5bda-489f-821c-4e7e...</td>\n",
       "      <td>e828e177-21ab-4a57-8a85-49789d028ff2</td>\n",
       "      <td>basic_search</td>\n",
       "      <td>gather ye rosebuds while ye may</td>\n",
       "      <td>fastly-default:0007f21549b981eec2f85e013f260023</td>\n",
       "      <td>5bef5573e2f944869b2cd9da411228ec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2023-08-30</td>\n",
       "      <td>True</td>\n",
       "      <td>Searches_Platform_271f3c55-45b6-4f5f-9327-5549...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>basic_search</td>\n",
       "      <td>nicolaas van der waay dutch painter fin de siècle</td>\n",
       "      <td>fastly-default:00094239ffc0c12e40e10b2ec18941ce</td>\n",
       "      <td>8a6f7a9476934f01b8baa39abd2967fd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2023-08-21</td>\n",
       "      <td>False</td>\n",
       "      <td>Searches_Platform_1659c830-ca36-499b-bfe7-b176...</td>\n",
       "      <td>3bd26986-75de-4dfc-8c03-7847910d7353</td>\n",
       "      <td>basic_search</td>\n",
       "      <td>car pollution</td>\n",
       "      <td>fastly-default:000ff57e598d821597179d943571f3ce</td>\n",
       "      <td>3442c7b38f5148339a27c4fc0fca6f91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            dt  institutional_affiliation  \\\n",
       "24  2023-08-07                       True   \n",
       "25  2023-08-29                      False   \n",
       "26  2023-08-17                      False   \n",
       "27  2023-08-30                       True   \n",
       "28  2023-08-21                      False   \n",
       "\n",
       "                                            metric_id  \\\n",
       "24  Searches_Platform_cd11029c-37bf-42f5-8284-1d3f...   \n",
       "25  Searches_Platform_af5d01f0-58d4-4153-bda5-c915...   \n",
       "26  Searches_Platform_b1d6a3f7-5bda-489f-821c-4e7e...   \n",
       "27  Searches_Platform_271f3c55-45b6-4f5f-9327-5549...   \n",
       "28  Searches_Platform_1659c830-ca36-499b-bfe7-b176...   \n",
       "\n",
       "                     personal_account_id   search_type  \\\n",
       "24                                   NaN  basic_search   \n",
       "25  d30ed7fe-6536-4261-9b1c-5d127ca2390e  basic_search   \n",
       "26  e828e177-21ab-4a57-8a85-49789d028ff2  basic_search   \n",
       "27                                   NaN  basic_search   \n",
       "28  3bd26986-75de-4dfc-8c03-7847910d7353  basic_search   \n",
       "\n",
       "                                         query_string  \\\n",
       "24                                        child abuse   \n",
       "25  ahmad n hassan r effects external funding sust...   \n",
       "26                    gather ye rosebuds while ye may   \n",
       "27  nicolaas van der waay dutch painter fin de siècle   \n",
       "28                                      car pollution   \n",
       "\n",
       "                                          requestid  \\\n",
       "24  fastly-default:00032f2cdcc9e112559ce0f44451d869   \n",
       "25  fastly-default:0003926a918f974401fce8e9993c3767   \n",
       "26  fastly-default:0007f21549b981eec2f85e013f260023   \n",
       "27  fastly-default:00094239ffc0c12e40e10b2ec18941ce   \n",
       "28  fastly-default:000ff57e598d821597179d943571f3ce   \n",
       "\n",
       "                      auth_sessionid  \n",
       "24  df2c17513cb64824b246e84cea5475e3  \n",
       "25  89098123304e450295305f421b93f438  \n",
       "26  5bef5573e2f944869b2cd9da411228ec  \n",
       "27  8a6f7a9476934f01b8baa39abd2967fd  \n",
       "28  3442c7b38f5148339a27c4fc0fca6f91  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_length(query):\n",
    "    if len(query) > 5:\n",
    "        return \"long\"\n",
    "    return \"short\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence-transformers) (4.34.1)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence-transformers) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence-transformers) (0.16.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence-transformers) (1.25.2)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence-transformers) (1.11.3)\n",
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence-transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence-transformers) (0.17.3)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.4)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (3.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.0)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk->sentence-transformers) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchvision->sentence-transformers) (10.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/fionaaaaa_mei/Documents/GitHub/JSTOR/JSTOR.ipynb Cell 14\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fionaaaaa_mei/Documents/GitHub/JSTOR/JSTOR.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m sentences \u001b[39m=\u001b[39m sentences[\u001b[39m0\u001b[39m:\u001b[39m10000\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fionaaaaa_mei/Documents/GitHub/JSTOR/JSTOR.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m SentenceTransformer(\u001b[39m'\u001b[39m\u001b[39msentence-transformers/all-mpnet-base-v2\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/fionaaaaa_mei/Documents/GitHub/JSTOR/JSTOR.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m embeddings \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mencode(sentences)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fionaaaaa_mei/Documents/GitHub/JSTOR/JSTOR.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(embeddings)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:165\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    162\u001b[0m features \u001b[39m=\u001b[39m batch_to_device(features, device)\n\u001b[1;32m    164\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 165\u001b[0m     out_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(features)\n\u001b[1;32m    167\u001b[0m     \u001b[39mif\u001b[39;00m output_value \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtoken_embeddings\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    168\u001b[0m         embeddings \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sentence_transformers/models/Transformer.py:66\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m features:\n\u001b[1;32m     64\u001b[0m     trans_features[\u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m features[\u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 66\u001b[0m output_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mauto_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtrans_features, return_dict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     67\u001b[0m output_tokens \u001b[39m=\u001b[39m output_states[\u001b[39m0\u001b[39m]\n\u001b[1;32m     69\u001b[0m features\u001b[39m.\u001b[39mupdate({\u001b[39m'\u001b[39m\u001b[39mtoken_embeddings\u001b[39m\u001b[39m'\u001b[39m: output_tokens, \u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m: features[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]})\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/mpnet/modeling_mpnet.py:551\u001b[0m, in \u001b[0;36mMPNetModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    550\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(input_ids\u001b[39m=\u001b[39minput_ids, position_ids\u001b[39m=\u001b[39mposition_ids, inputs_embeds\u001b[39m=\u001b[39minputs_embeds)\n\u001b[0;32m--> 551\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    552\u001b[0m     embedding_output,\n\u001b[1;32m    553\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    554\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    555\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    556\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    557\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    558\u001b[0m )\n\u001b[1;32m    559\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    560\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/mpnet/modeling_mpnet.py:341\u001b[0m, in \u001b[0;36mMPNetEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[39mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    339\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (hidden_states,)\n\u001b[0;32m--> 341\u001b[0m layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    342\u001b[0m     hidden_states,\n\u001b[1;32m    343\u001b[0m     attention_mask,\n\u001b[1;32m    344\u001b[0m     head_mask[i],\n\u001b[1;32m    345\u001b[0m     position_bias,\n\u001b[1;32m    346\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    347\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    348\u001b[0m )\n\u001b[1;32m    349\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    351\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/mpnet/modeling_mpnet.py:311\u001b[0m, in \u001b[0;36mMPNetLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, position_bias, output_attentions, **kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m outputs \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add self attentions if we output attention weights\u001b[39;00m\n\u001b[1;32m    310\u001b[0m intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 311\u001b[0m layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput(intermediate_output, attention_output)\n\u001b[1;32m    312\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[1;32m    313\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/mpnet/modeling_mpnet.py:278\u001b[0m, in \u001b[0;36mMPNetOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor, input_tensor: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 278\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(hidden_states)\n\u001b[1;32m    279\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    280\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(hidden_states \u001b[39m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = df['query_string'].tolist()\n",
    "sentences = sentences[0:10000]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000024e+00,  6.00022115e-02,  6.10139631e-02, ...,\n",
       "         1.11107536e-01,  2.83833772e-01,  3.36013049e-01],\n",
       "       [ 6.00022115e-02,  1.00000012e+00, -6.97791800e-02, ...,\n",
       "         1.65645376e-01,  2.48845425e-02,  1.87191740e-02],\n",
       "       [ 6.10139631e-02, -6.97791800e-02,  1.00000000e+00, ...,\n",
       "        -1.68380309e-02,  1.09538764e-01,  5.54632694e-02],\n",
       "       ...,\n",
       "       [ 1.11107536e-01,  1.65645376e-01, -1.68380309e-02, ...,\n",
       "         1.00000000e+00, -4.15358692e-04, -2.49168091e-02],\n",
       "       [ 2.83833772e-01,  2.48845425e-02,  1.09538764e-01, ...,\n",
       "        -4.15358692e-04,  1.00000024e+00,  6.93451762e-02],\n",
       "       [ 3.36013049e-01,  1.87191740e-02,  5.54632694e-02, ...,\n",
       "        -2.49168091e-02,  6.93451762e-02,  1.00000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity = similarity = cosine_similarity(embeddings, embeddings)\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {}\n",
    "for i in range(len(similarity)):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embeddings:\n",
      "tensor([[-1.3081e-02, -3.0887e-03,  9.9356e-03,  2.5945e-02,  1.7544e-02,\n",
      "          1.7259e-02, -8.9481e-03,  9.2967e-03,  4.0810e-03,  2.1503e-02,\n",
      "         -8.4300e-03,  6.5115e-02,  2.3292e-02, -7.3931e-03,  6.6806e-03,\n",
      "          4.0605e-03, -5.5216e-03, -6.0035e-03,  3.8435e-02, -4.3108e-03,\n",
      "          1.2339e-02,  1.7104e-02,  2.2402e-02,  4.8841e-02,  6.3238e-02,\n",
      "          8.6374e-03, -1.9316e-02,  2.8212e-02, -4.8609e-02,  9.0360e-03,\n",
      "          1.9076e-02,  8.5810e-03,  1.6533e-02, -7.6406e-03,  1.7538e-06,\n",
      "          1.5360e-02, -2.8907e-02,  1.5209e-03,  1.3755e-02, -2.0205e-02,\n",
      "          2.8680e-02, -1.3209e-02, -2.0372e-02, -1.5390e-02,  7.3954e-04,\n",
      "         -6.6736e-03,  4.8000e-02, -4.9933e-02, -5.6546e-02,  2.5208e-02,\n",
      "          3.1560e-03, -1.6832e-02, -9.6703e-03,  9.1974e-03,  3.2618e-02,\n",
      "          1.6189e-02,  3.4066e-03, -3.1438e-04,  1.7028e-02,  3.4267e-02,\n",
      "         -3.5247e-02, -3.0119e-03, -3.5470e-02, -1.8167e-02,  1.1946e-02,\n",
      "          2.9224e-02, -4.5010e-03,  4.7340e-02,  4.8708e-02, -3.5022e-02,\n",
      "         -9.8289e-02, -2.8248e-02,  4.3073e-02,  1.2405e-01, -3.1277e-02,\n",
      "          1.0799e-02,  7.5272e-03,  1.3245e-02, -4.7211e-02,  2.1987e-02,\n",
      "         -7.5268e-02,  5.3985e-03,  1.3158e-02,  2.4491e-02,  1.5115e-02,\n",
      "          4.1850e-02,  2.0983e-02,  4.5510e-03, -3.4984e-02,  5.0393e-03,\n",
      "          1.7151e-02, -3.7568e-02, -4.9209e-03,  5.6508e-02, -6.7326e-02,\n",
      "          1.8287e-03,  1.8665e-02,  5.2395e-02, -1.0295e-02, -2.0530e-02,\n",
      "          2.4600e-02,  4.2243e-02, -1.6634e-02, -9.8367e-04, -4.5452e-02,\n",
      "          4.1868e-03,  4.6639e-02,  4.2977e-02, -2.1373e-03,  3.9295e-03,\n",
      "         -7.5628e-03, -1.8599e-02, -2.5507e-02,  7.4890e-02,  3.8054e-02,\n",
      "          6.3877e-02, -1.5201e-02,  3.4577e-02, -1.4640e-02,  2.6454e-02,\n",
      "          2.5223e-02,  1.5786e-02,  1.2178e-02,  8.1520e-03,  2.6303e-02,\n",
      "         -1.5672e-02, -6.8720e-02,  1.4882e-02,  4.0046e-02, -1.3133e-02,\n",
      "         -3.1403e-02,  5.3638e-03, -1.2154e-02, -3.3378e-02,  3.8603e-02,\n",
      "         -6.4810e-02, -2.7078e-02, -4.5874e-02, -1.9024e-02,  2.4006e-03,\n",
      "         -5.0887e-02, -3.5934e-02,  1.5307e-02, -1.9122e-02, -1.8124e-02,\n",
      "          1.6652e-03,  1.5029e-02, -1.0243e-01,  1.1084e-03,  1.9208e-02,\n",
      "          1.7018e-02, -1.4586e-02, -1.6221e-02,  2.0270e-02,  7.1812e-02,\n",
      "         -8.1805e-03, -2.8832e-02, -1.2737e-02, -1.7880e-02,  1.9984e-02,\n",
      "         -9.3354e-03, -8.2507e-03, -1.8939e-02, -3.0459e-02,  1.7851e-02,\n",
      "          9.4426e-04,  7.3342e-03,  4.6010e-02, -5.4561e-02, -6.0082e-03,\n",
      "         -2.3346e-02,  3.2706e-02, -1.0902e-02, -7.3475e-02,  4.3468e-02,\n",
      "          2.0135e-02, -1.8842e-02,  1.3348e-02,  2.9277e-04,  2.3278e-02,\n",
      "          4.7320e-03, -2.1983e-01,  1.8943e-02, -3.4208e-02, -6.5896e-02,\n",
      "          2.3017e-02,  4.3435e-03,  3.3126e-02, -4.0129e-02, -2.7925e-02,\n",
      "          4.0065e-02, -4.2982e-03, -3.5139e-02,  2.3675e-02, -5.3260e-03,\n",
      "          3.0983e-02, -1.8698e-02, -1.0250e-01, -1.0365e-02,  4.8212e-02,\n",
      "         -6.4562e-02, -5.8065e-02, -1.1083e-02, -3.9086e-02, -1.2085e-02,\n",
      "          5.7119e-02,  1.5763e-03, -4.9126e-03, -4.9803e-02, -3.4442e-02,\n",
      "          4.1762e-03,  2.6560e-02,  1.0425e-02,  1.6912e-02,  5.1508e-03,\n",
      "          3.7877e-02,  4.0814e-02,  1.5152e-03, -2.7757e-03, -1.3357e-02,\n",
      "          8.4407e-03,  6.1109e-03,  5.1452e-02, -3.1090e-02, -1.0278e-02,\n",
      "          1.3484e-02, -4.6150e-03, -1.2944e-02, -1.7875e-02, -1.4948e-02,\n",
      "          3.9285e-03,  2.9243e-02,  2.8016e-02,  2.7046e-02,  4.5235e-02,\n",
      "          2.1724e-02,  8.5284e-02, -2.3154e-02, -2.4356e-02, -2.2615e-02,\n",
      "          3.1295e-02,  3.1424e-02,  1.2000e-03, -1.3467e-03,  4.0067e-02,\n",
      "          2.3171e-02,  5.8991e-02,  1.5626e-02,  2.7683e-02,  7.5575e-03,\n",
      "          7.1685e-03, -5.0804e-03,  8.0805e-02,  1.4610e-02,  4.7379e-02,\n",
      "          1.0806e-01, -3.2212e-02,  1.2153e-02, -6.0372e-02,  1.3726e-02,\n",
      "         -4.6923e-02, -4.5013e-02,  5.1644e-03, -3.7152e-02,  3.4250e-02,\n",
      "         -1.5118e-03,  9.0265e-03, -2.6600e-02,  1.5984e-02, -4.6386e-03,\n",
      "         -1.2945e-02,  2.2209e-03,  2.5114e-02, -1.7603e-02,  1.0439e-03,\n",
      "          1.6177e-02, -1.2820e-01,  1.0257e-02, -1.8739e-03, -6.8143e-03,\n",
      "          1.5014e-02,  1.4896e-02, -2.2316e-02, -1.7295e-03,  1.6472e-03,\n",
      "          4.4378e-02, -1.0014e-02,  1.3743e-02, -4.9901e-02,  1.5049e-03,\n",
      "         -4.4765e-02, -2.9005e-02,  2.3474e-02,  2.1834e-03,  8.4141e-03,\n",
      "         -1.7615e-02,  1.0468e-02, -1.8784e-02,  1.6334e-02, -3.0028e-02,\n",
      "         -3.2587e-02, -3.5883e-02,  5.7464e-03, -2.7675e-02, -3.9632e-02,\n",
      "         -7.3099e-02, -8.4294e-03, -7.0244e-03,  1.8161e-02, -2.9195e-03,\n",
      "         -9.5488e-03,  2.5463e-02,  1.2314e-02, -6.5433e-02,  2.9902e-02,\n",
      "          4.7571e-02,  5.8828e-02, -3.3449e-02,  1.1045e-02,  2.6070e-02,\n",
      "         -2.3777e-02,  3.5936e-02, -1.6192e-03, -4.2963e-02, -2.8122e-02,\n",
      "          4.8709e-02, -3.5144e-02, -1.0926e-02, -8.6299e-02,  2.1632e-02,\n",
      "          2.0594e-02,  1.4910e-02, -1.6692e-02,  1.2694e-02, -3.4763e-02,\n",
      "         -9.0065e-02, -1.3527e-02, -2.8856e-02,  2.0142e-02,  2.6821e-02,\n",
      "          1.2333e-02, -6.0591e-02, -2.8110e-02,  1.3440e-02, -3.2155e-02,\n",
      "         -2.7303e-02,  3.6826e-02,  4.5833e-02,  3.1997e-02,  2.2809e-02,\n",
      "         -1.9318e-02, -1.8143e-04,  8.9910e-03, -3.9635e-02, -1.3666e-02,\n",
      "         -2.1043e-02,  6.1281e-03,  4.4318e-03, -1.3314e-02,  2.1781e-02,\n",
      "         -5.4824e-02, -6.4731e-04,  7.9110e-02, -6.7767e-02, -5.2583e-02,\n",
      "          3.2411e-02,  2.1829e-02,  1.5919e-04,  2.5260e-02, -3.1936e-02,\n",
      "         -4.8997e-02,  4.9407e-02, -7.3714e-02,  1.8763e-02, -1.6957e-02,\n",
      "         -1.0986e-01,  1.7717e-03,  6.5225e-02,  9.0182e-02, -2.0198e-02,\n",
      "          7.9842e-02,  2.9800e-02, -5.7644e-03,  1.0137e-02,  1.5632e-02,\n",
      "         -6.3242e-02, -5.3054e-02, -2.1987e-02, -7.8710e-02, -5.5375e-03,\n",
      "          3.1980e-02, -1.3639e-02, -1.0425e-01,  3.9974e-02,  3.5602e-02,\n",
      "          3.5784e-02, -1.3156e-02,  2.6060e-02, -3.7528e-02, -1.5136e-03,\n",
      "         -6.0768e-03,  1.5235e-02,  3.2008e-02,  9.3673e-03,  5.0887e-03,\n",
      "          2.5788e-02, -3.9865e-02, -2.9934e-02,  3.1838e-02, -5.0125e-02,\n",
      "         -3.1522e-02,  5.1112e-02,  4.4380e-02,  5.6466e-03, -7.5784e-02,\n",
      "         -6.0136e-02,  7.0780e-04, -1.4365e-02,  1.7852e-02, -5.4575e-02,\n",
      "          3.2247e-02, -2.2569e-02, -2.8756e-02, -1.3414e-02,  6.3128e-02,\n",
      "          1.2224e-02,  5.1799e-02,  2.9546e-02, -3.1536e-03,  3.7717e-02,\n",
      "          2.2496e-02, -1.0181e-02, -3.4653e-02,  1.4922e-02, -1.2524e-03,\n",
      "         -1.5363e-01,  1.0312e-02,  6.9177e-02,  4.2777e-03,  4.5398e-02,\n",
      "          1.0736e-02,  5.1359e-03, -7.8208e-02, -1.9519e-02, -2.7934e-02,\n",
      "         -1.4646e-02, -9.5482e-04, -2.3942e-02, -3.2782e-03, -6.6136e-02,\n",
      "         -1.2054e-02,  1.1755e-02,  6.2261e-03, -3.1382e-03, -3.1523e-02,\n",
      "         -1.6300e-02,  4.2283e-02, -1.0874e-01,  2.5840e-02,  4.8255e-02,\n",
      "          6.5087e-02,  3.4691e-02,  7.3700e-04, -3.1723e-02,  5.6496e-03,\n",
      "         -3.9844e-03, -7.7083e-02,  4.8403e-03, -1.4234e-02, -3.8548e-02,\n",
      "         -6.2029e-02,  3.2894e-03,  5.8993e-03, -2.1025e-02,  6.1410e-02,\n",
      "         -2.7587e-02, -2.1227e-03, -7.6859e-02,  9.0985e-02,  1.1169e-02,\n",
      "         -2.6779e-02,  1.7948e-02,  3.4672e-02,  4.8646e-03,  5.9676e-04,\n",
      "         -6.2080e-03,  6.8693e-03,  4.1948e-02,  2.5125e-02,  1.6231e-02,\n",
      "         -3.7328e-02, -9.5787e-03,  1.3675e-02, -7.0418e-02, -3.8433e-03,\n",
      "          9.5292e-03, -3.0408e-02, -1.0787e-01,  3.7859e-02,  3.2141e-02,\n",
      "          1.9868e-02,  3.7229e-02,  2.0372e-02,  7.7578e-02,  3.1351e-02,\n",
      "          1.3022e-02, -4.7134e-03, -4.5066e-02,  3.0719e-02,  1.2521e-02,\n",
      "         -4.0800e-03,  9.2659e-03, -1.9025e-02, -2.0861e-02,  4.3043e-03,\n",
      "          8.0151e-02,  1.3416e-02,  6.0228e-02,  2.3044e-03,  2.6028e-03,\n",
      "          1.4587e-02, -2.9090e-02,  4.2117e-02,  3.7926e-02, -8.1476e-02,\n",
      "         -4.8953e-02,  1.5773e-02, -1.0788e-02,  3.9944e-02,  2.5186e-02,\n",
      "         -1.6492e-02, -1.6672e-02,  3.2003e-02, -2.2058e-02, -4.1105e-02,\n",
      "          6.9592e-02, -3.4258e-02,  2.3821e-02,  1.7851e-03, -6.3909e-02,\n",
      "         -5.8722e-02, -1.8416e-02, -1.7250e-02,  1.1165e-02,  1.5578e-02,\n",
      "          2.5861e-02, -2.3208e-02, -3.2076e-02, -7.4539e-03, -2.0327e-02,\n",
      "         -2.7171e-02,  6.0901e-02, -1.0503e-01, -3.4388e-02,  3.0505e-04,\n",
      "         -6.3220e-33,  1.3099e-03, -3.0335e-02, -1.3458e-02,  6.2391e-02,\n",
      "         -2.3567e-02, -4.9420e-02, -2.3650e-04,  3.2644e-02, -3.5238e-02,\n",
      "         -1.6738e-02, -2.5552e-02,  1.8632e-02,  3.2349e-02,  1.7803e-02,\n",
      "          2.9066e-02, -6.6221e-03, -1.0648e-02,  1.0208e-02,  6.9259e-03,\n",
      "          7.2342e-03, -4.6819e-02,  1.9679e-02, -6.5523e-03,  1.0169e-02,\n",
      "          2.0600e-02, -5.8756e-02, -8.0831e-03,  5.1993e-02,  2.8763e-02,\n",
      "          5.5948e-02,  1.6525e-02,  3.7528e-02, -2.6745e-02,  4.4113e-02,\n",
      "          1.7553e-02, -7.7029e-03, -9.3061e-03, -1.3519e-03, -2.2908e-02,\n",
      "         -3.2605e-02, -5.6491e-02, -2.9143e-02,  3.1730e-02, -4.5125e-03,\n",
      "         -1.2106e-02,  4.3696e-02,  1.3937e-02, -2.6605e-02, -1.7071e-02,\n",
      "          2.0730e-02, -3.9192e-03,  1.0001e-02, -4.5165e-03, -4.3466e-02,\n",
      "          3.5733e-02,  3.5026e-02,  1.8719e-02,  1.0885e-01, -1.6931e-02,\n",
      "         -5.3738e-02,  3.0294e-03, -3.2967e-02, -2.1762e-02,  3.9329e-02,\n",
      "         -1.7351e-02, -6.1256e-03,  6.2826e-02,  3.7074e-02,  5.0107e-02,\n",
      "          5.0602e-02, -3.6174e-02,  6.3981e-02, -2.5849e-02, -7.7474e-02,\n",
      "         -3.5565e-02, -8.1078e-03, -2.3806e-02,  1.4440e-02,  7.5468e-02,\n",
      "         -4.2821e-02, -1.0596e-02, -8.6020e-03, -1.7291e-02, -3.3237e-03,\n",
      "          2.9001e-02, -3.9600e-03, -2.3625e-02, -2.7302e-02, -1.4566e-02,\n",
      "          3.6156e-02,  7.6749e-03,  1.2934e-01,  1.7138e-02,  2.6116e-04,\n",
      "         -1.4459e-02, -3.2427e-02, -3.7299e-03, -4.6396e-02, -8.8989e-03,\n",
      "         -3.7982e-03,  4.1601e-02, -3.4505e-03, -1.6218e-02,  2.1338e-02,\n",
      "          9.8480e-03, -5.0633e-03,  6.2046e-02,  4.0134e-03, -3.4750e-02,\n",
      "         -1.6945e-03, -3.5488e-02,  1.0746e-02, -2.5052e-02,  8.2844e-02,\n",
      "          9.0577e-03,  1.8711e-03,  1.1551e-03,  2.8813e-02,  1.8069e-03,\n",
      "          6.6726e-02,  4.7987e-02, -7.4998e-04,  8.2146e-04, -1.6799e-02,\n",
      "         -1.0175e-02,  1.8154e-02,  4.0130e-02,  4.7345e-02,  2.4024e-02,\n",
      "          3.9957e-02, -7.7608e-04, -4.5924e-02,  2.4674e-07,  5.5140e-02,\n",
      "          1.7871e-03,  5.3683e-03,  1.0982e-03, -1.3615e-02, -8.3652e-04,\n",
      "          1.5610e-02,  3.1456e-02,  1.9771e-02,  6.3828e-02,  1.8859e-02,\n",
      "         -2.1904e-02,  4.2736e-02,  2.9181e-02, -2.0656e-02, -9.0393e-02,\n",
      "         -1.0878e-05,  1.1530e-02,  7.9214e-03,  1.6440e-02,  6.9369e-02,\n",
      "         -8.9325e-03,  2.4350e-02, -2.3562e-02,  2.0976e-02, -4.5377e-02,\n",
      "          1.3174e-02, -4.3777e-02, -3.5754e-02, -1.9186e-02,  6.8337e-02,\n",
      "         -5.8278e-03, -9.8342e-03, -2.8613e-02,  5.0269e-02, -1.9661e-02,\n",
      "          2.4152e-02,  1.4842e-02, -3.3637e-04, -1.6802e-02, -2.4775e-02,\n",
      "         -5.3384e-03, -1.1346e-02, -2.9090e-03, -2.3603e-02, -9.5965e-03,\n",
      "          7.0757e-03,  1.3464e-02, -6.1470e-02, -3.9222e-02, -2.8480e-02,\n",
      "         -7.0645e-03, -5.0269e-03, -1.3123e-02,  1.5270e-02,  1.9196e-02,\n",
      "         -5.7125e-03, -3.9494e-02, -1.4915e-02, -4.4067e-02, -2.1585e-02,\n",
      "          2.1746e-02, -1.7263e-02, -2.9827e-02,  4.2626e-02, -4.9176e-02,\n",
      "          3.4341e-03,  1.9189e-34, -8.4808e-03, -5.1320e-03,  6.0098e-03,\n",
      "          7.6336e-03, -1.3148e-02,  1.3422e-02, -1.6590e-02, -8.4874e-03,\n",
      "         -1.6153e-02, -4.6455e-02, -1.1855e-02]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Perform pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# Normalize embeddings\n",
    "sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "print(\"Sentence embeddings:\")\n",
    "print(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1., -1., ...,  1.,  1.,  1.],\n",
       "       [ 1.,  1., -1., ...,  1.,  1.,  1.],\n",
       "       [-1., -1.,  1., ..., -1., -1., -1.],\n",
       "       ...,\n",
       "       [ 1.,  1., -1., ...,  1.,  1.,  1.],\n",
       "       [ 1.,  1., -1., ...,  1.,  1.,  1.],\n",
       "       [ 1.,  1., -1., ...,  1.,  1.,  1.]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity = similarity = cosine_similarity(embeddings, embeddings)\n",
    "similarity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
